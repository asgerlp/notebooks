{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import tarfile\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()  # this could also be a hard-coded bucket name\n",
    "\n",
    "print(\"Using bucket \" + bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the California housing dataset\n",
    "data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "trainX[\"target\"] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "testX[\"target\"] = y_test\n",
    "trainX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.to_csv(\"california_housing_train.csv\")\n",
    "testX.to_csv(\"california_housing_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce409cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to S3. SageMaker will take training data from s3\n",
    "trainpath = sess.upload_data(\n",
    "    path=\"california_housing_train.csv\", bucket=bucket, key_prefix=\"sagemaker/sklearncontainer\"\n",
    ")\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path=\"california_housing_test.csv\", bucket=bucket, key_prefix=\"sagemaker/sklearncontainer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbe3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "datarobot\n",
    "https://asgerlp-public.s3.eu-central-1.amazonaws.com/datarobot_mlops-7.3.4-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7925b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import datarobot as dr\n",
    "from datarobot.mlops.mlops import MLOps\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# inference functions ---------------\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    #initialize MLOps\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"<----INSERT KEY---->\"\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"]=\"<----INSERT KEY---->\"\n",
    "    os.environ[\"AWS_SESSION_TOKEN\"]=\"<----INSERT KEY---->\"\n",
    "\n",
    "    DEPLOYMENT_ID = '<----INSERT ID---->'\n",
    "    MODEL_ID = '<----INSERT ID---->'\n",
    "    SQS_QUEUE = '<----INSERT SQS URL ---->'\n",
    "\n",
    "    mlops = MLOps() \\\n",
    "    .set_deployment_id(DEPLOYMENT_ID) \\\n",
    "    .set_model_id(MODEL_ID) \\\n",
    "    .set_sqs_spooler(SQS_QUEUE)\n",
    "    \n",
    "    mlops.init()\n",
    "\n",
    "    # Pass values to the model\n",
    "    start_time = time.time()\n",
    "    prediction = model.predict(input_data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    mlops.report_deployment_stats(1, end_time - start_time)\n",
    "    \n",
    "    #mlops.report_predictions_data(features_df=features_df, predictions=prediction.tolist())\n",
    "\n",
    "    \n",
    "    return np.array([prediction])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"california_housing_train.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"california_housing_test.csv\")\n",
    "    parser.add_argument(\n",
    "        \"--features\", type=str\n",
    "    )  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str\n",
    "    )  # in this script we ask user to explicitly name the target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"reading data\")\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print(\"building training and testing datasets\")\n",
    "    X_train = train_df[args.features.split()]\n",
    "    X_test = test_df[args.features.split()]\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "\n",
    "    # train\n",
    "    print(\"training model\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators, min_samples_leaf=args.min_samples_leaf, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # print abs error\n",
    "    print(\"validating model\")\n",
    "    abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "\n",
    "    # print couple perf metrics\n",
    "    for q in [10, 50, 90]:\n",
    "        print(\"AE-at-\" + str(q) + \"th-percentile: \" + str(np.percentile(a=abs_err, q=q)))\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print(\"model persisted at \" + path)\n",
    "    print(args.min_samples_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"rf-scikit\",\n",
    "    metric_definitions=[{\"Name\": \"median-AE\", \"Regex\": \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": 100,\n",
    "        \"min-samples-leaf\": 3,\n",
    "        \"features\": \"MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude\",\n",
    "        \"target\": \"target\",\n",
    "    },\n",
    "    source_dir = '.',\n",
    "    env = {\n",
    "    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aaa4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch training job, with asynchronous call\n",
    "sklearn_estimator.fit({\"train\": trainpath, \"test\": testpath}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator.latest_training_job.wait(logs=\"None\")\n",
    "artifact = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=sklearn_estimator.latest_training_job.name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(\"Model artifact persisted at \" + artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "model = SKLearnModel(\n",
    "    model_data=artifact,\n",
    "    role=get_execution_role(),\n",
    "    entry_point=\"script.py\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    source_dir = '.',\n",
    "    env = {\n",
    "    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d999eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(instance_type=\"ml.c5.large\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44400ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the SKLearnPredictor does the serialization from pandas for us\n",
    "print(predictor.predict(testX[data.feature_names]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40147f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3.delete_endpoint(EndpointName=predictor.endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbf3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.predict(testX[data.feature_names]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ffad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
